# Overview
- This enhanced case study provides a comprehensive overview of Ziko Logistics' approach to establishing a state-of-the-art data engineering practice that aligns with its operational needs and strategic goals.

# Objectives:
- To automate data processes, ensure high data quality, and provide scalable solutions for data storage and retrieval.
  
# Benefits:
- Achieving these objectives will lead to faster data processing, reduced downtime, enhanced data security, and better support for predictive analytics and decision-making.


# Tech Stack
## Tools:
- Python is used for its versatility in data manipulation and machine learning.
- SQL is essential for querying large datasets efficiently.
- Azure Data Lake Gen 2 provides a highly scalable and secure storage solution.
- GitHub facilitates code sharing and version control.
- Task Scheduler automates the ETL process, ensuring operations run at optimal times.




# Project Scope
## Project Scope:
- Data Extraction: Automated scripts pull data from historical records ensuring that data ingestion is consistent and reliable.
- Data Transformation: Utilizes advanced algorithms to cleanse, reformat, and prepare data for analytical use, adhering to 2NF or 3NF database normalization rules.
- Data Loading: Ensures that data is securely stored in Azure, with changes tracked through Git for version control, while orchestration via Windows Task Scheduler allows for efficient management of data workflows.

